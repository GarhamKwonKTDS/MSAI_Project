# nodes/solution_delivery.py

import logging
from typing import Dict, Any, Optional
from langchain_openai import AzureChatOpenAI
from models.state import ChatbotState, update_state_metadata
from services.azure_search import search_service

logger = logging.getLogger(__name__)

def solution_delivery_node(state: ChatbotState, config: Dict[str, Any], llm: AzureChatOpenAI) -> ChatbotState:
    """
    ì†”ë£¨ì…˜ ì œê³µ ë…¸ë“œ - í™•ì •ëœ ì¼€ì´ìŠ¤ì— ëŒ€í•œ ê°œì¸í™”ëœ í•´ê²°ì±… ìƒì„± ë° ì œê³µ
    
    í”„ë¡œì„¸ìŠ¤:
    1. í™•ì •ëœ ì¼€ì´ìŠ¤ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
    2. ìˆ˜ì§‘ëœ ì •ë³´ì™€ ëŒ€í™” ë§¥ë½ ë¶„ì„
    3. ê°œì¸í™”ëœ ì†”ë£¨ì…˜ ìƒì„±
    4. í›„ì† ì¡°ì¹˜ ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì˜µì…˜ í¬í•¨
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        config: ëŒ€í™” ì„¤ì • ì •ë³´
        llm: Azure OpenAI LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        ChatbotState: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    state = update_state_metadata(state, "solution_delivery")
    
    logger.info(f"ğŸ¯ Solution Delivery")
    logger.info(f"   Issue: {state['current_issue']}")
    logger.info(f"   Case: {state['current_case']}")
    logger.info(f"   Gathered Info: {len(state['gathered_info'])} items")
    
    try:
        # 1. ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        case_details = _get_case_details(state)
        
        if not case_details:
            logger.error("   âŒ Could not retrieve case details")
            state['needs_escalation'] = True
            state['escalation_reason'] = "case_details_not_found"
            state['final_response'] = config['fallback_responses']['escalation']
            return state
        
        # 2. ê°œì¸í™”ëœ ì†”ë£¨ì…˜ ìƒì„±
        solution = _generate_personalized_solution(
            state, 
            case_details, 
            config, 
            llm
        )
        
        if solution:
            # 3. í›„ì† ì¡°ì¹˜ ë° ì¶”ê°€ ì˜µì…˜ í¬í•¨
            final_response = _format_final_response(
                solution, 
                case_details, 
                state, 
                config
            )
            
            state['final_response'] = final_response
            state['resolution_attempted'] = True
            
            logger.info(f"   âœ… Solution delivered ({len(final_response)} chars)")
            
        else:
            logger.error("   âŒ Failed to generate solution")
            state['needs_escalation'] = True
            state['escalation_reason'] = "solution_generation_failed"
            state['final_response'] = config['fallback_responses']['escalation']
        
    except Exception as e:
        logger.error(f"   âŒ Solution delivery error: {e}")
        state['error_count'] += 1
        state['needs_escalation'] = True
        state['escalation_reason'] = "solution_delivery_error"
        state['final_response'] = config['fallback_responses']['general_error']
    
    return state

def _get_case_details(state: ChatbotState) -> Optional[Dict[str, Any]]:
    """
    í™•ì •ëœ ì¼€ì´ìŠ¤ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        
    Returns:
        Optional[Dict]: ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´
    """
    
    if not state['current_case']:
        return None
    
    # 1. ë¨¼ì € ì´ë¯¸ ê²€ìƒ‰ëœ ì¼€ì´ìŠ¤ë“¤ì—ì„œ ì°¾ê¸°
    for case in state.get('retrieved_cases', []):
        case_id = case.get('case_type') or case.get('id')
        if case_id == state['current_case']:
            logger.info(f"   ğŸ“‹ Found case details in retrieved cases")
            return case
    
    # 2. Azure Searchì—ì„œ ì§ì ‘ ì¡°íšŒ
    try:
        case_details = search_service.get_case_by_id(state['current_case'])
        if case_details:
            logger.info(f"   ğŸ“‹ Retrieved case details from search")
            return case_details
    except Exception as e:
        logger.error(f"Error retrieving case details: {e}")
    
    # 3. ì´ìŠˆ íƒ€ì…ìœ¼ë¡œ í•„í„°ë§í•´ì„œ ë‹¤ì‹œ ì°¾ê¸°
    if state['current_issue']:
        cases = search_service.filter_cases_by_issue_type(
            query=state['user_message'],
            issue_type=state['current_issue'],
            top_k=10
        )
        
        for case in cases:
            case_id = case.get('case_type') or case.get('id')
            if case_id == state['current_case']:
                logger.info(f"   ğŸ“‹ Found case details by filtering")
                return case
    
    logger.warning(f"   âš ï¸ Could not find details for case: {state['current_case']}")
    return None

def _generate_personalized_solution(
    state: ChatbotState, 
    case_details: Dict[str, Any], 
    config: Dict[str, Any], 
    llm: AzureChatOpenAI
) -> Optional[str]:
    """
    ê°œì¸í™”ëœ ì†”ë£¨ì…˜ ìƒì„±
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        case_details: ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´
        config: ì„¤ì • ì •ë³´
        llm: LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        Optional[str]: ìƒì„±ëœ ì†”ë£¨ì…˜
    """
    
    # ì‚¬ìš©ì ìƒí™© ì •ë³´ êµ¬ì„±
    user_context = _build_user_context(state)
    
    # ì¼€ì´ìŠ¤ í•´ê²° ë‹¨ê³„ ì •ë³´
    solution_steps = case_details.get('solution_steps', [])
    escalation_triggers = case_details.get('escalation_triggers', [])
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_template = config['conversation_flow']['solution_delivery']['prompt_template']
    
    full_prompt = f"""
{prompt_template.format(
    case=case_details.get('case_name', state['current_case']),
    gathered_info=user_context
)}

ì¼€ì´ìŠ¤ ì •ë³´:
- ì´ë¦„: {case_details.get('case_name', '')}
- ì„¤ëª…: {case_details.get('description', '')}

í‘œì¤€ í•´ê²° ë‹¨ê³„:
{chr(10).join([f"{i}. {step}" for i, step in enumerate(solution_steps, 1)])}

ì—ìŠ¤ì»¬ë ˆì´ì…˜ ìƒí™©:
- {chr(10).join(escalation_triggers)}

ì‘ë‹µ ìš”êµ¬ì‚¬í•­:
- í†¤: {config['response_formatting']['tone']}
- ì–¸ì–´ ìŠ¤íƒ€ì¼: {config['response_formatting']['language_style']}
- ìµœëŒ€ ê¸¸ì´: {config['response_formatting']['max_response_length']}ì
- ë‹¨ê³„ ë²ˆí˜¸ í¬í•¨: {config['response_formatting']['include_step_numbers']}
- ì„¤ëª… í¬í•¨: {config['response_formatting']['include_explanations']}

ì‚¬ìš©ìì˜ êµ¬ì²´ì ì¸ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ê°œì¸í™”ëœ í•´ê²° ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”.
ê° ë‹¨ê³„ì— ëŒ€í•´ ì™œ ê·¸ ë‹¨ê³„ê°€ í•„ìš”í•œì§€ ê°„ë‹¨íˆ ì„¤ëª…í•˜ê³ ,
ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ë”°ë¼í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­ì„ ì œê³µí•˜ì„¸ìš”.
"""
    
    try:
        response = llm.invoke(full_prompt)
        solution = response.content.strip()
        
        # ê¸¸ì´ ì œí•œ ì²´í¬
        max_length = config['response_formatting']['max_response_length']
        if len(solution) > max_length:
            # ê¸¸ì´ê°€ ì´ˆê³¼í•˜ë©´ ìš”ì•½ ë²„ì „ ìƒì„±
            solution = _summarize_solution(solution, max_length, llm)
        
        return solution
        
    except Exception as e:
        logger.error(f"Solution generation error: {e}")
        return None

def _build_user_context(state: ChatbotState) -> str:
    """
    ì‚¬ìš©ì ìƒí™© ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        
    Returns:
        str: ì‚¬ìš©ì ìƒí™© ì»¨í…ìŠ¤íŠ¸
    """
    
    context_parts = []
    
    # ì´ˆê¸° ë¬¸ì œ ìƒí™©
    context_parts.append(f"ì´ˆê¸° ë¬¸ì˜: {state['user_message']}")
    
    # ìˆ˜ì§‘ëœ êµ¬ì²´ì ì¸ ì •ë³´
    if state['gathered_info']:
        context_parts.append("\nì‚¬ìš©ì ìƒí™©:")
        for key, info in state['gathered_info'].items():
            if isinstance(info, dict):
                question = info.get('question', '')
                answer = info.get('answer', '')
                if question and answer:
                    context_parts.append(f"- {question}: {answer}")
            else:
                context_parts.append(f"- {key}: {info}")
    
    # ëŒ€í™” ì¤‘ ì–¸ê¸‰ëœ ì¶”ê°€ ì •ë³´
    if state['conversation_history']:
        recent_context = []
        for turn in state['conversation_history'][-2:]:  # ìµœê·¼ 2í„´
            user_msg = turn.get('user', '')
            if user_msg and user_msg != state['user_message']:
                recent_context.append(user_msg)
        
        if recent_context:
            context_parts.append(f"\nì¶”ê°€ ì–¸ê¸‰ì‚¬í•­: {' / '.join(recent_context)}")
    
    return "\n".join(context_parts)

def _format_final_response(
    solution: str, 
    case_details: Dict[str, Any], 
    state: ChatbotState, 
    config: Dict[str, Any]
) -> str:
    """
    ìµœì¢… ì‘ë‹µ í¬ë§·íŒ… (í›„ì† ì¡°ì¹˜ ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì˜µì…˜ í¬í•¨)
    
    Args:
        solution: ìƒì„±ëœ ì†”ë£¨ì…˜
        case_details: ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        config: ì„¤ì • ì •ë³´
        
    Returns:
        str: í¬ë§·íŒ…ëœ ìµœì¢… ì‘ë‹µ
    """
    
    response_parts = [solution]
    
    # í›„ì† ì¡°ì¹˜ ì „ëµ ì¶”ê°€
    if config['conversation_flow']['solution_delivery']['follow_up_strategy']:
        follow_up = config['conversation_flow']['solution_delivery']['follow_up_strategy']
        response_parts.append(f"\n\n{follow_up}")
    
    # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì˜µì…˜ í¬í•¨
    if config['conversation_flow']['solution_delivery']['include_escalation_option']:
        escalation_triggers = case_details.get('escalation_triggers', [])
        if escalation_triggers:
            response_parts.append("\n\nğŸš¨ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš° ì¶”ê°€ ì§€ì›ì´ í•„ìš”í•©ë‹ˆë‹¤:")
            for trigger in escalation_triggers[:2]:  # ìµœëŒ€ 2ê°œë§Œ
                response_parts.append(f"- {trigger}")
            response_parts.append("\nì´ëŸ° ìƒí™©ì´ ë°œìƒí•˜ë©´ ì „ë¬¸ ìƒë‹´ì›ì—ê²Œ ì—°ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
    
    # ì¶”ê°€ ë„ì›€ ì•ˆë‚´
    response_parts.append("\n\nğŸ’¡ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ë‹¤ë¥¸ ë¬¸ì œê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!")
    
    return "".join(response_parts)

def _summarize_solution(solution: str, max_length: int, llm: AzureChatOpenAI) -> str:
    """
    ì†”ë£¨ì…˜ ìš”ì•½ (ê¸¸ì´ ì´ˆê³¼ì‹œ)
    
    Args:
        solution: ì›ë³¸ ì†”ë£¨ì…˜
        max_length: ìµœëŒ€ ê¸¸ì´
        llm: LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        str: ìš”ì•½ëœ ì†”ë£¨ì…˜
    """
    
    prompt = f"""
ë‹¤ìŒ í•´ê²° ë°©ë²•ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì•¼ í•©ë‹ˆë‹¤.
í•µì‹¬ ë‹¨ê³„ì™€ ì¤‘ìš”í•œ ì •ë³´ë§Œ í¬í•¨í•˜ì—¬ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ì›ë³¸:
{solution}

ìš”ì•½ ({max_length}ì ì´ë‚´):
"""
    
    try:
        response = llm.invoke(prompt)
        summarized = response.content.strip()
        
        # ì—¬ì „íˆ ê¸¸ë©´ ê°•ì œë¡œ ìë¥´ê¸°
        if len(summarized) > max_length:
            summarized = summarized[:max_length-3] + "..."
        
        return summarized
        
    except Exception as e:
        logger.error(f"Solution summarization error: {e}")
        # ì‹¤íŒ¨ì‹œ ì›ë³¸ì„ ê°•ì œë¡œ ìë¥´ê¸°
        return solution[:max_length-3] + "..."