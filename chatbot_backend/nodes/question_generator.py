# nodes/question_generator.py

import logging
from typing import Dict, Any, List, Optional
from langchain_openai import AzureChatOpenAI
from models.state import ChatbotState, update_state_metadata
from services.azure_search import search_service

logger = logging.getLogger(__name__)

def question_generation_node(state: ChatbotState, config: Dict[str, Any], llm: AzureChatOpenAI) -> ChatbotState:
    """
    ì§ˆë¬¸ ìƒì„± ë…¸ë“œ - ì¼€ì´ìŠ¤ í™•ì •ì„ ìœ„í•œ ì ì ˆí•œ ì§ˆë¬¸ ìƒì„±
    
    í”„ë¡œì„¸ìŠ¤:
    1. ìµœëŒ€ ì§ˆë¬¸ ìˆ˜ ì²´í¬ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    2. í˜„ì¬ ì´ìŠˆ/ì¼€ì´ìŠ¤ í›„ë³´ë“¤ì—ì„œ ê´€ë ¨ ì§ˆë¬¸ë“¤ ìˆ˜ì§‘
    3. ì´ë¯¸ ë¬»ì§€ ì•Šì€ ì§ˆë¬¸ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ì§ˆë¬¸ ì„ íƒ
    4. LLMìœ¼ë¡œ ë§¥ë½ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ ìƒì„±
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        config: ëŒ€í™” ì„¤ì • ì •ë³´
        llm: Azure OpenAI LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        ChatbotState: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    state = update_state_metadata(state, "question_generation")
    
    logger.info(f"â“ Question Generation - Question #{state['question_count'] + 1}")
    logger.info(f"   Current Issue: {state['current_issue']}")
    logger.info(f"   Questions Asked: {len(state['questions_asked'])}")
    
    try:
        # 1. ìµœëŒ€ ì§ˆë¬¸ ìˆ˜ ì²´í¬
        max_questions = config['conversation_flow']['case_narrowing']['max_questions_per_case']
        if state['question_count'] >= max_questions:
            logger.warning(f"   âš ï¸ Max questions reached ({max_questions}) - escalating")
            state['needs_escalation'] = True
            state['escalation_reason'] = "max_questions_exceeded"
            state['final_response'] = config['fallback_responses']['escalation']
            return state
        
        # 2. ê´€ë ¨ ì§ˆë¬¸ë“¤ ìˆ˜ì§‘
        candidate_questions = _collect_candidate_questions(state, config)
        
        if not candidate_questions:
            logger.warning("   âš ï¸ No candidate questions found - escalating")
            state['needs_escalation'] = True
            state['escalation_reason'] = "no_questions_available"
            state['final_response'] = config['fallback_responses']['escalation']
            return state
        
        # 3. ìµœì  ì§ˆë¬¸ ì„ íƒ
        selected_question = _select_best_question(
            state, 
            candidate_questions, 
            config, 
            llm
        )
        
        if selected_question:
            # 4. ì§ˆë¬¸ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ê¸°
            final_question = _refine_question(
                selected_question, 
                state, 
                config, 
                llm
            )
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state['final_response'] = final_question
            state['questions_asked'].append(selected_question)
            state['question_count'] += 1
            
            logger.info(f"   âœ… Question generated: {final_question[:100]}...")
            
        else:
            logger.warning("   âš ï¸ Could not select appropriate question - escalating")
            state['needs_escalation'] = True
            state['escalation_reason'] = "question_selection_failed"
            state['final_response'] = config['fallback_responses']['escalation']
        
    except Exception as e:
        logger.error(f"   âŒ Question generation error: {e}")
        state['error_count'] += 1
        
        if state['error_count'] < 3:
            state['final_response'] = config['fallback_responses']['need_more_info']
        else:
            state['needs_escalation'] = True
            state['escalation_reason'] = "too_many_errors"
            state['final_response'] = config['fallback_responses']['escalation']
    
    return state

def _collect_candidate_questions(state: ChatbotState, config: Dict[str, Any]) -> List[str]:
    """
    í›„ë³´ ì§ˆë¬¸ë“¤ ìˆ˜ì§‘
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        config: ì„¤ì • ì •ë³´
        
    Returns:
        List[str]: í›„ë³´ ì§ˆë¬¸ë“¤
    """
    
    candidate_questions = []
    
    # 1. Azure Searchì—ì„œ ê´€ë ¨ ì§ˆë¬¸ë“¤ ê°€ì ¸ì˜¤ê¸°
    if state['current_issue']:
        search_questions = search_service.get_related_questions(
            issue_type=state['current_issue'],
            case_type=state.get('current_case')
        )
        candidate_questions.extend(search_questions)
    
    # 2. ê²€ìƒ‰ëœ ì¼€ì´ìŠ¤ë“¤ì—ì„œ ì§ˆë¬¸ ì¶”ì¶œ
    for case in state.get('retrieved_cases', []):
        case_questions = case.get('questions_to_ask', [])
        candidate_questions.extend(case_questions)
    
    # 3. ì¤‘ë³µ ì œê±° ë° ì´ë¯¸ ë¬¼ì–´ë³¸ ì§ˆë¬¸ í•„í„°ë§
    unique_questions = []
    asked_questions_lower = [q.lower() for q in state['questions_asked']]
    
    for question in candidate_questions:
        if question and question.lower() not in asked_questions_lower:
            if question not in unique_questions:
                unique_questions.append(question)
    
    logger.info(f"   ğŸ“‹ Found {len(unique_questions)} candidate questions")
    return unique_questions

def _select_best_question(
    state: ChatbotState, 
    questions: List[str], 
    config: Dict[str, Any], 
    llm: AzureChatOpenAI
) -> Optional[str]:
    """
    ê°€ì¥ ì í•©í•œ ì§ˆë¬¸ ì„ íƒ
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        questions: í›„ë³´ ì§ˆë¬¸ë“¤
        config: ì„¤ì • ì •ë³´
        llm: LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        Optional[str]: ì„ íƒëœ ì§ˆë¬¸ ë˜ëŠ” None
    """
    
    if not questions:
        return None
    
    # ì§ˆë¬¸ì´ í•˜ë‚˜ë¿ì´ë©´ ë°”ë¡œ ë°˜í™˜
    if len(questions) == 1:
        return questions[0]
    
    # ì „ëµì— ë”°ë¥¸ ì§ˆë¬¸ ì„ íƒ
    selection_strategy = config['conversation_flow']['case_narrowing'].get('question_selection_strategy', 'progressive')
    
    if selection_strategy == 'progressive':
        return _select_progressive_question(state, questions, llm)
    else:
        # ê¸°ë³¸ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ì§ˆë¬¸ ì„ íƒ
        return questions[0]

def _select_progressive_question(
    state: ChatbotState, 
    questions: List[str], 
    llm: AzureChatOpenAI
) -> Optional[str]:
    """
    ì ì§„ì  ì „ëµìœ¼ë¡œ ì§ˆë¬¸ ì„ íƒ - LLMì´ ë§¥ë½ì— ê°€ì¥ ì í•©í•œ ì§ˆë¬¸ ì„ íƒ
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        questions: í›„ë³´ ì§ˆë¬¸ë“¤
        llm: LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        Optional[str]: ì„ íƒëœ ì§ˆë¬¸
    """
    
    # ëŒ€í™” ë§¥ë½ êµ¬ì„±
    context = _build_question_context(state)
    
    # ì§ˆë¬¸ ëª©ë¡ êµ¬ì„±
    question_list = []
    for i, question in enumerate(questions[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ
        question_list.append(f"{i}. {question}")
    
    prompt = f"""
ë‹¤ìŒì€ í˜„ì¬ ëŒ€í™” ìƒí™©ì…ë‹ˆë‹¤:
{context}

ì‚¬ìš©ìì˜ ë¬¸ì œë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì¤‘ ê°€ì¥ ì ì ˆí•œ ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”:

{chr(10).join(question_list)}

ì„ íƒ ê¸°ì¤€:
- í˜„ì¬ ìƒí™©ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì§ˆë¬¸
- ì‚¬ìš©ìê°€ ì‰½ê²Œ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
- ì¼€ì´ìŠ¤ í™•ì •ì— ê°€ì¥ ë„ì›€ì´ ë˜ëŠ” ì§ˆë¬¸

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
ì„ íƒ: [ë²ˆí˜¸]
ì´ìœ : [ì„ íƒ ì´ìœ ]
"""
    
    try:
        response = llm.invoke(prompt)
        result = response.content.strip()
        
        # ì‘ë‹µì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ
        lines = result.split('\n')
        for line in lines:
            if line.strip().startswith('ì„ íƒ:'):
                try:
                    number = int(line.replace('ì„ íƒ:', '').strip())
                    if 1 <= number <= len(questions):
                        selected_question = questions[number - 1]
                        logger.info(f"   ğŸ¯ LLM selected question #{number}")
                        return selected_question
                except ValueError:
                    pass
        
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì²« ë²ˆì§¸ ì§ˆë¬¸ ë°˜í™˜
        logger.warning("   âš ï¸ Failed to parse LLM selection, using first question")
        return questions[0]
        
    except Exception as e:
        logger.error(f"Question selection error: {e}")
        return questions[0]

def _build_question_context(state: ChatbotState) -> str:
    """
    ì§ˆë¬¸ ì„ íƒì„ ìœ„í•œ ëŒ€í™” ë§¥ë½ êµ¬ì„±
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        
    Returns:
        str: ëŒ€í™” ë§¥ë½
    """
    
    context_parts = []
    
    context_parts.append(f"ë¬¸ì œ ìœ í˜•: {state['current_issue']}")
    context_parts.append(f"ì‚¬ìš©ì ë©”ì‹œì§€: {state['user_message']}")
    
    if state['gathered_info']:
        context_parts.append("ì´ë¯¸ ìˆ˜ì§‘ëœ ì •ë³´:")
        for key, info in state['gathered_info'].items():
            if isinstance(info, dict):
                context_parts.append(f"- {info.get('question', '')}: {info.get('answer', '')}")
    
    if state['answers_received']:
        context_parts.append(f"ìµœê·¼ ë‹µë³€: {state['answers_received'][-1]}")
    
    return "\n".join(context_parts)

def _refine_question(
    question: str, 
    state: ChatbotState, 
    config: Dict[str, Any], 
    llm: AzureChatOpenAI
) -> str:
    """
    ì§ˆë¬¸ì„ ìì—°ìŠ¤ëŸ½ê³  ë§¥ë½ì— ë§ê²Œ ë‹¤ë“¬ê¸°
    
    Args:
        question: ì›ë³¸ ì§ˆë¬¸
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        config: ì„¤ì • ì •ë³´
        llm: LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        str: ë‹¤ë“¬ì–´ì§„ ì§ˆë¬¸
    """
    
    # ì„¤ì •ì—ì„œ ì‘ë‹µ ìŠ¤íƒ€ì¼ ê°€ì ¸ì˜¤ê¸°
    tone = config['response_formatting']['tone']
    language_style = config['response_formatting']['language_style']
    
    gathering_strategy = config['conversation_flow']['case_narrowing']['information_gathering_strategy']
    
    prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ë” ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ë„ë¡ ë‹¤ë“¬ì–´ ì£¼ì„¸ìš”:

ì›ë³¸ ì§ˆë¬¸: "{question}"

ìš”êµ¬ì‚¬í•­:
- í†¤: {tone}
- ì–¸ì–´ ìŠ¤íƒ€ì¼: {language_style}
- ì „ëµ: {gathering_strategy}

í˜„ì¬ ìƒí™©: ì‚¬ìš©ìê°€ {state['current_issue']} ë¬¸ì œë¥¼ ê²ªê³  ìˆìœ¼ë©°, {state['question_count']}ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.

ë‹¤ë“¬ì–´ì§„ ì§ˆë¬¸:
"""
    
    try:
        response = llm.invoke(prompt)
        refined_question = response.content.strip()
        
        # ê¸°ë³¸ ê²€ì¦ (ë„ˆë¬´ ê¸¸ë©´ ì›ë³¸ ì‚¬ìš©)
        if len(refined_question) > 200:
            return question
            
        return refined_question
        
    except Exception as e:
        logger.error(f"Question refinement error: {e}")
        return question