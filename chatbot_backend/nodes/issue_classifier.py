# nodes/issue_classifier.py

import logging
from typing import Dict, Any, List
from langchain_openai import AzureChatOpenAI
from models.state import ChatbotState, update_state_metadata
from services.azure_search import search_service

logger = logging.getLogger(__name__)

def issue_classification_node(state: ChatbotState, config: Dict[str, Any], llm: AzureChatOpenAI) -> ChatbotState:
    """
    ì´ìŠˆ ë¶„ë¥˜ ë…¸ë“œ - Azure Searchë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì´ìŠˆ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
    
    í”„ë¡œì„¸ìŠ¤:
    1. Azure Searchë¡œ ê´€ë ¨ ì¼€ì´ìŠ¤ë“¤ ê²€ìƒ‰
    2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìŠˆ íƒ€ì… ì¶”ì¶œ
    3. LLMìœ¼ë¡œ ì´ìŠˆ ë¶„ë¥˜ í™•ì¸ ë° ì‹ ë¢°ë„ í‰ê°€
    4. ë¶„ë¥˜ ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥
    
    Args:
        state: í˜„ì¬ ì±—ë´‡ ìƒíƒœ
        config: ëŒ€í™” ì„¤ì • ì •ë³´
        llm: Azure OpenAI LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        ChatbotState: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    state = update_state_metadata(state, "issue_classification")
    state['classification_attempts'] += 1
    
    logger.info(f"ğŸ·ï¸ Issue Classification - Attempt {state['classification_attempts']}")
    logger.info(f"   User Message: {state['user_message'][:100]}...")
    
    try:
        # 1. Azure Searchë¡œ ê´€ë ¨ ì¼€ì´ìŠ¤ ê²€ìƒ‰
        search_query = _build_search_query(state['user_message'])
        state['search_query'] = search_query
        
        retrieved_cases = search_service.search_cases(search_query, top_k=5)
        state['retrieved_cases'] = retrieved_cases
        
        if retrieved_cases:
            state['rag_used'] = True
            logger.info(f"   ğŸ” Found {len(retrieved_cases)} relevant cases")
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì´ìŠˆ íƒ€ì…ë“¤ ì¶”ì¶œ
            issue_types = _extract_issue_types_from_search(retrieved_cases)
            
            # RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            rag_context = search_service.build_rag_context(retrieved_cases)
            state['rag_context'] = rag_context
            
        else:
            logger.warning("   âš ï¸ No relevant cases found in search")
            issue_types = []
            state['rag_used'] = False
        
        # 2. LLMì„ ì‚¬ìš©í•œ ì´ìŠˆ ë¶„ë¥˜
        classified_issue, confidence = _classify_with_llm(
            state['user_message'], 
            issue_types, 
            state.get('rag_context', ''),
            config, 
            llm
        )
        
        # 3. ë¶„ë¥˜ ê²°ê³¼ ê²€ì¦ ë° ì €ì¥
        confidence_threshold = config['conversation_flow']['issue_classification']['confidence_threshold']
        
        if classified_issue and confidence >= confidence_threshold:
            state['current_issue'] = classified_issue
            state['classification_confidence'] = confidence
            logger.info(f"   âœ… Issue classified: {classified_issue} (confidence: {confidence:.2f})")
            
        else:
            state['classification_confidence'] = confidence
            logger.info(f"   â“ Low confidence classification: {classified_issue} ({confidence:.2f})")
            
            # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ëª…í™•í™” ì§ˆë¬¸ ìƒì„±
            if state['classification_attempts'] < config['conversation_flow']['issue_classification']['max_classification_attempts']:
                clarification = _generate_clarification_question(state['user_message'], issue_types, config, llm)
                state['final_response'] = clarification
                logger.info("   ğŸ“ Generated clarification question")
            else:
                # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ì‹œ ì—ìŠ¤ì»¬ë ˆì´ì…˜
                state['needs_escalation'] = True
                state['escalation_reason'] = "classification_failed"
                state['final_response'] = config['fallback_responses']['classification_unclear']
                logger.info("   âš ï¸ Max classification attempts reached - escalating")
        
    except Exception as e:
        logger.error(f"   âŒ Issue classification error: {e}")
        state['error_count'] += 1
        state['classification_confidence'] = 0.0
        
        if state['error_count'] < 3:
            state['final_response'] = config['fallback_responses']['general_error']
        else:
            state['needs_escalation'] = True
            state['escalation_reason'] = "too_many_errors"
            state['final_response'] = config['fallback_responses']['escalation']
    
    return state

def _build_search_query(user_message: str) -> str:
    """
    ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
    
    Args:
        user_message: ì‚¬ìš©ì ë©”ì‹œì§€
        
    Returns:
        str: ê²€ìƒ‰ ì¿¼ë¦¬
    """
    # ê°„ë‹¨í•œ ì „ì²˜ë¦¬ (í–¥í›„ ë” ì •êµí•œ ì¿¼ë¦¬ êµ¬ì„± ê°€ëŠ¥)
    query = user_message.strip()
    
    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
    stop_phrases = ["ì•ˆë…•í•˜ì„¸ìš”", "ë„ì™€ì£¼ì„¸ìš”", "ë¬¸ì œê°€ ìˆì–´ìš”", "ë¬¸ì œê°€ ìƒê²¼ì–´ìš”"]
    for phrase in stop_phrases:
        query = query.replace(phrase, "")
    
    return query.strip()

def _extract_issue_types_from_search(cases: List[Dict[str, Any]]) -> List[str]:
    """
    ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì´ìŠˆ íƒ€ì…ë“¤ ì¶”ì¶œ
    
    Args:
        cases: ê²€ìƒ‰ëœ ì¼€ì´ìŠ¤ë“¤
        
    Returns:
        List[str]: ê³ ìœ í•œ ì´ìŠˆ íƒ€ì…ë“¤
    """
    issue_types = []
    for case in cases:
        issue_type = case.get('issue_type')
        if issue_type and issue_type not in issue_types:
            issue_types.append(issue_type)
    
    return issue_types

def _classify_with_llm(
    user_message: str, 
    issue_types: List[str], 
    rag_context: str,
    config: Dict[str, Any], 
    llm: AzureChatOpenAI
) -> tuple[str, float]:
    """
    LLMì„ ì‚¬ìš©í•œ ì´ìŠˆ ë¶„ë¥˜
    
    Args:
        user_message: ì‚¬ìš©ì ë©”ì‹œì§€
        issue_types: ê°€ëŠ¥í•œ ì´ìŠˆ íƒ€ì…ë“¤
        rag_context: RAG ì»¨í…ìŠ¤íŠ¸
        config: ì„¤ì • ì •ë³´
        llm: LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        tuple: (ë¶„ë¥˜ëœ ì´ìŠˆ, ì‹ ë¢°ë„)
    """
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_template = config['conversation_flow']['issue_classification']['prompt_template']
    
    if issue_types and rag_context:
        full_prompt = f"""
{prompt_template.format(user_message=user_message)}

ê²€ìƒ‰ëœ ê´€ë ¨ ì¼€ì´ìŠ¤ ì •ë³´:
{rag_context}

ê°€ëŠ¥í•œ ì´ìŠˆ íƒ€ì…ë“¤:
{', '.join(issue_types)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
ì´ìŠˆíƒ€ì…: [ê°€ì¥ ì í•©í•œ ì´ìŠˆ íƒ€ì…]
ì‹ ë¢°ë„: [0.0-1.0 ì‚¬ì´ì˜ ìˆ«ì]
ì´ìœ : [ë¶„ë¥˜ ê·¼ê±°]

í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ 'ë¶ˆëª…í™•'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
"""
    else:
        full_prompt = f"""
{prompt_template.format(user_message=user_message)}

ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ì¼ë°˜ì ì¸ ë¶„ë¥˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
ì´ìŠˆíƒ€ì…: [ì¶”ì •ë˜ëŠ” ì´ìŠˆ íƒ€ì… ë˜ëŠ” 'ë¶ˆëª…í™•']
ì‹ ë¢°ë„: [0.0-1.0 ì‚¬ì´ì˜ ìˆ«ì]
ì´ìœ : [ë¶„ë¥˜ ê·¼ê±°]
"""
    
    try:
        response = llm.invoke(full_prompt)
        result = response.content.strip()
        
        # ì‘ë‹µ íŒŒì‹±
        issue_type, confidence = _parse_classification_response(result)
        
        return issue_type, confidence
        
    except Exception as e:
        logger.error(f"LLM classification error: {e}")
        return None, 0.0

def _parse_classification_response(response: str) -> tuple[str, float]:
    """
    LLM ë¶„ë¥˜ ì‘ë‹µ íŒŒì‹±
    
    Args:
        response: LLM ì‘ë‹µ
        
    Returns:
        tuple: (ì´ìŠˆíƒ€ì…, ì‹ ë¢°ë„)
    """
    
    lines = response.split('\n')
    issue_type = None
    confidence = 0.0
    
    for line in lines:
        line = line.strip()
        if line.startswith('ì´ìŠˆíƒ€ì…:'):
            issue_type = line.replace('ì´ìŠˆíƒ€ì…:', '').strip()
            if issue_type == 'ë¶ˆëª…í™•':
                issue_type = None
        elif line.startswith('ì‹ ë¢°ë„:'):
            try:
                confidence_str = line.replace('ì‹ ë¢°ë„:', '').strip()
                confidence = float(confidence_str)
            except ValueError:
                confidence = 0.0
    
    return issue_type, confidence

def _generate_clarification_question(
    user_message: str, 
    issue_types: List[str], 
    config: Dict[str, Any], 
    llm: AzureChatOpenAI
) -> str:
    """
    ëª…í™•í™” ì§ˆë¬¸ ìƒì„±
    
    Args:
        user_message: ì‚¬ìš©ì ë©”ì‹œì§€
        issue_types: ê°€ëŠ¥í•œ ì´ìŠˆ íƒ€ì…ë“¤
        config: ì„¤ì • ì •ë³´
        llm: LLM ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        str: ëª…í™•í™” ì§ˆë¬¸
    """
    
    if not issue_types:
        return config['fallback_responses']['classification_unclear']
    
    prompt = f"""
ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"

ê°€ëŠ¥í•œ ë¬¸ì œ ìœ í˜•ë“¤: {', '.join(issue_types)}

ì‚¬ìš©ìì˜ ë¬¸ì œë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê¸° ìœ„í•œ ëª…í™•í™” ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ
- êµ¬ì²´ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ
- í•œ ë²ˆì— í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ

ì§ˆë¬¸:
"""
    
    try:
        response = llm.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        logger.error(f"Clarification question generation error: {e}")
        return config['fallback_responses']['need_more_info']